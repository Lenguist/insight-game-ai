{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Follows this tutorial\n",
    "# UNFINISHED\n",
    "https://gymnasium.farama.org/tutorials/training_agents/blackjack_tutorial/#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from matplotlib) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: seaborn in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from seaborn) (1.24.3)\n",
      "Requirement already satisfied: pandas>=0.25 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from seaborn) (2.0.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from seaborn) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gymnasium in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from gymnasium) (1.24.3)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from gymnasium) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from gymnasium) (4.6.3)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/mbondarenko/opt/anaconda3/envs/gym-env/lib/python3.11/site-packages (from gymnasium) (0.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install tqdm\n",
    "%pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Till Zemann\n",
    "# License: MIT License\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "# Let's start by creating the blackjack environment.\n",
    "# Note: We are going to follow the rules from Sutton & Barto.\n",
    "# Other versions of the game can be found below for you to experiment.\n",
    "\n",
    "env = gym.make(\"Blackjack-v1\", sab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment to get the first observation\n",
    "done = False\n",
    "observation, info = env.reset()\n",
    "\n",
    "# observation = (16, 9, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample a random action from all valid actions\n",
    "action = env.action_space.sample()\n",
    "# action=1\n",
    "\n",
    "# execute the action in our environment and receive infos from the environment\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# observation=(24, 10, False)\n",
    "# reward=-1.0\n",
    "# terminated=True\n",
    "# truncated=False\n",
    "# info={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackjackAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95,\n",
    "    ):\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        # with probability epsilon return a random action to explore the environment\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "\n",
    "        # with probability (1 - epsilon) act greedily (exploit)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs: tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: tuple[int, int, bool],\n",
    "    ):\n",
    "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        temporal_difference = (\n",
    "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
    "        )\n",
    "\n",
    "        self.q_values[obs][action] = (\n",
    "            self.q_values[obs][action] + self.lr * temporal_difference\n",
    "        )\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
